{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44f7ffa-0349-4726-be45-aad0b70970ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import time \n",
    "import os \n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc16b48-a3a8-49fd-8b95-2f8f9514dfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=cv2.imread(r\"C:\\Users\\pc\\OneDrive\\Desktop\\smart board\\tool_bar.png\")\n",
    "image = cv2.resize(image, (1280, 125))\n",
    "image = cv2.flip(image, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41067fb-970d-4a6a-adfb-fa693eee4858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 1280, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc115bc-07c0-46ac-bc65-4d0f306ea71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db05c5ab-1729-43e7-9541-3acedd769019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c460c54f-265d-4e5a-9556-19f1cb0312d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor =TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-handwritten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f5d7c4-ed56-4ce7-b7dd-840f8e34f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"pooler_act\": \"tanh\",\n",
      "  \"pooler_output_size\": 1024,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 1024,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-handwritten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88aae452-022b-4a4e-bdd8-ac2c4aae1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "    display(Image.fromarray(img)) \n",
    "    return img\n",
    "\n",
    "def ocr_image(image):\n",
    "    image = Image.fromarray(image) # Convert to PIL image\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7461f9e8-59b5-4e2b-bebd-54ebc2c78a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def solve_equation(equation_str):\n",
    "    try:\n",
    "        equation_str = equation_str.replace(\"x\", \"*\")\n",
    "        equation_str = equation_str.replace(\"X\", \"*\")\n",
    "        equation_str = equation_str.replace(\"=\", \"==\")  \n",
    "        numbers_operators = re.findall(r\"(\\d+\\.?\\d*|\\+|-|\\*|/|=|==)\", equation_str)\n",
    "        result = 0.0\n",
    "        current_op = \"+\"\n",
    "        for item in numbers_operators:\n",
    "            if item.isdigit() or (\".\" in item and item.replace(\".\", \"\").isdigit()):\n",
    "                num = float(item)\n",
    "                if current_op == \"+\":\n",
    "                    result += num\n",
    "                elif current_op == \"-\":\n",
    "                    result -= num\n",
    "                elif current_op == \"*\":\n",
    "                    result *= num\n",
    "                elif current_op == \"/\":\n",
    "                    if num == 0:\n",
    "                        return \"Division by zero error\"\n",
    "                    result /= num\n",
    "            elif item in (\"+\", \"-\", \"*\", \"/\"):\n",
    "                current_op = item\n",
    "            elif item == \"=\" or item == \"==\":\n",
    "                pass  \n",
    "        return result\n",
    "\n",
    "    except (ValueError, TypeError, IndexError, AttributeError):\n",
    "        return \"Invalid equation\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "752715d2-90fd-4d0d-a79e-b80c69cb18e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Clear\n",
      "Eraser\n",
      "Eraser\n",
      "Solve\n",
      "OCR Result: 00 5x3 .\n",
      "Solution: 15.0\n",
      "Solve\n",
      "OCR Result: 00 5x3\n",
      "Solution: 15.0\n",
      "Solve\n",
      "OCR Result: 00 5x3\n",
      "Solution: 15.0\n",
      "Solve\n",
      "OCR Result: 00 5x3\n",
      "Solution: 15.0\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "#cap.set()\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)  # Width\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)  # Height\n",
    "color=(0,0,255)\n",
    "thickness=15\n",
    "eraser_thickness=100\n",
    "xp,yp=0,0\n",
    "\n",
    "drwaing_img=np.zeros((720,1280,3),np.uint8)\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands: \n",
    "    while True:\n",
    "        success,img=cap.read()\n",
    "        img[0:125,0:1280]=image\n",
    "    \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.flip(img, 1)\n",
    "        \n",
    "        img.flags.writeable = False\n",
    "        results = hands.process(img)\n",
    "        img.flags.writeable = True\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        #print(results)\n",
    "        \n",
    "        lmlist=[]\n",
    "        tipids = [4, 8, 12, 16, 20]\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                mp_drawing.draw_landmarks(img, hand, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    for id, lm in enumerate(hand_landmarks.landmark):\n",
    "                        h, w, c = img.shape  # Get image dimensions\n",
    "                        cx, cy = int(lm.x * w), int(lm.y * h)  # Convert normalized coordinates to pixel values\n",
    "                        lmlist.append((id, cx, cy))  # Append landmark ID and position\n",
    "                \n",
    "        '''    \n",
    "        if len(lmlist)!=0:\n",
    "            #print(lmlist)\n",
    "            print(lmlist[8])\n",
    "            x1,y1=lmlist[8][1:]\n",
    "            x2,y2=lmlist[12][1:]\n",
    "        '''\n",
    "        \n",
    "        if len(lmlist) != 0:\n",
    "            fingers = []\n",
    "            \n",
    "            # Thumb (compare x-coordinates because thumb moves horizontally)\n",
    "            if lmlist[tipids[0]][1] < lmlist[tipids[0] - 1][1]:\n",
    "                fingers.append(1)\n",
    "            else:\n",
    "                fingers.append(0)\n",
    "            \n",
    "            # Other four fingers (compare y-coordinates because they move vertically)\n",
    "            for id in range(1, 5):\n",
    "                if lmlist[tipids[id]][2] < lmlist[tipids[id] - 2][2]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "            \n",
    "            #print(f\"Fingers: {fingers}\")\n",
    "            x1, y1 = lmlist[8][1], lmlist[8][2]\n",
    "            x2, y2 = lmlist[12][1], lmlist[12][2]\n",
    "\n",
    "            if fingers[1] and fingers[2]:\n",
    "                xp,yp=0,0\n",
    "                #print(\"Selection Mode\")\n",
    "                if y1<125:\n",
    "                    if 280<x1<378:\n",
    "                        print(\"Pencil\")\n",
    "                        color=(0,0,255)\n",
    "                    elif  490<x1<655:\n",
    "                        print(\"Eraser\")\n",
    "                        color=(0,0,0)\n",
    "                    elif 785<x1<970:\n",
    "                        print(\"Clear\")\n",
    "                        drwaing_img = np.zeros((720, 1280, 3), np.uint8)\n",
    "                        #color=(0,0,255)\n",
    "                    elif 1000<x1<1190:\n",
    "                        print(\"Solve\")\n",
    "                        #color=(0,255,0)\n",
    "                        ocr_img = drwaing_img.copy()\n",
    "                        ocr_img_gray = cv2.cvtColor(ocr_img, cv2.COLOR_BGR2GRAY)\n",
    "                        _, ocr_img_thresh = cv2.threshold(ocr_img_gray, 50, 255, cv2.THRESH_BINARY)\n",
    "                        ocr_img_rgb = cv2.cvtColor(ocr_img_thresh, cv2.COLOR_GRAY2RGB)\n",
    "                        pil_img = Image.fromarray(ocr_img_rgb)\n",
    "            \n",
    "                        # OCR\n",
    "                        pixel_values = processor(images=pil_img, return_tensors=\"pt\").pixel_values\n",
    "                        generated_ids = model.generate(pixel_values)\n",
    "                        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "                        print(f\"OCR Result: {generated_text}\")\n",
    "\n",
    "                        solution = solve_equation(generated_text)\n",
    "                        print(f\"Solution: {solution}\")\n",
    "\n",
    "                        # Optional: show on screen\n",
    "                        cv2.putText(img, f'{generated_text} = {solution}', (50, 680), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 3)\n",
    "                cv2.rectangle(img,(x1,y1-25),(x2,y2+25),color,cv2.FILLED)\n",
    "\n",
    "                        \n",
    "            else:\n",
    "                #print(\"Drewing Mode\")\n",
    "                cv2.circle(img,(x1,y1),15,color,cv2.FILLED)\n",
    "                if xp==0 and yp==0:\n",
    "                    xp,yp=x1,y1\n",
    "                if color==(0,0,0):\n",
    "                    cv2.line(img,(xp,yp),(x1,y1),color,eraser_thickness)\n",
    "                    cv2.line(drwaing_img,(xp,yp),(x1,y1),color,eraser_thickness)\n",
    "                else:\n",
    "                    cv2.line(img,(xp,yp),(x1,y1),color,thickness)\n",
    "                    cv2.line(drwaing_img,(xp,yp),(x1,y1),color,thickness)\n",
    "                xp,yp=x1,y1\n",
    "\n",
    "        \n",
    "        imgGray=cv2.cvtColor(drwaing_img,cv2.COLOR_BGR2GRAY)\n",
    "        _, imgInv=cv2.threshold(imgGray,50,255,cv2.THRESH_BINARY_INV)\n",
    "        imgInv=cv2.cvtColor(imgInv,cv2.COLOR_GRAY2BGR)\n",
    "        img=cv2.bitwise_and(img,imgInv)\n",
    "        img=cv2.bitwise_or(img,drwaing_img)\n",
    "        \n",
    "        #img=cv2.addWeighted(img,0.5,drwaing_img,0.5,0)    \n",
    "        cv2.imshow(\"Image\",img)\n",
    "        cv2.imshow(\"drwaing\",drwaing_img)\n",
    "        if cv2.waitKey(1) == 27:\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2a617-ae11-4909-b01f-c865607e5b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe643e-5713-436d-ad95-c38c9d1fa47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684ff99-c671-4111-a84b-2442755bab63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce72ad4-4014-428e-bab9-3c2b5aff3053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
